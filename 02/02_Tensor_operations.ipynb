{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhTm-wTJNJEJ"
   },
   "source": [
    "# Deep Learning & Applied AI\n",
    "\n",
    "# Tutorial 2: Tensors operations\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Tensors operations: broadcasting, (not)-elementwise operations, tensors contraction, einsum\n",
    "\n",
    "Our info:\n",
    "\n",
    "- Luca Moschella (moschella@di.uniroma.it)\n",
    "- Antonio Norelli (norelli@di.uniroma1.it)\n",
    "\n",
    "Course:\n",
    "\n",
    "- Website and notebooks will be available at [DLAI-s2-2020](https://erodola.github.io/DLAI-s2-2020/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHLO4Z-T_yxB"
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "You should familiarize with the [PyTorch Documentation](https://pytorch.org/docs/stable/) as it will greatly assist you.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRePt-K1_yw9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0B2Y47YJY97"
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "# Utility print function\n",
    "def print_arr(*arr: Union[torch.Tensor, np.ndarray], prefix: str = \"\") -> None:\n",
    "    \"\"\" Pretty print tensors, together with their shape and type\n",
    "    \n",
    "    :param arr: one or more tensors\n",
    "    :param prefix: prefix to use when printing the tensors\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"\\n\\n\".join(\n",
    "            f\"{prefix}{str(x)} <shape: {x.shape}> <dtype: {x.dtype}>\" for x in arr\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bie5eT1Md_FW"
   },
   "source": [
    "####Set torch and numpy random seeds for reproducibility\n",
    "If you are going to use a gpu, two further options must be set. (CuDNN is a library of CUDA for Deep Neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tGN_bJOcfd3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(0)\n",
    "\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUl8vYRv_yuG"
   },
   "source": [
    "### **Tensor operations**\n",
    "\n",
    "Functions that operate on tensors are often accessible in different ways, with the same meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFFU6Xw7_yuA"
   },
   "outputs": [],
   "source": [
    "t = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TKgi2HL_yt_"
   },
   "source": [
    "Operators **overload**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "HydKd9OK_yt7",
    "outputId": "a1377eee-67e8-49e3-bc74-8482f5bc4a84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7645, 1.8300, 0.7657],\n",
       "        [1.9186, 0.7809, 1.2018],\n",
       "        [0.5131, 1.5873, 1.8815]])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Amiof4a_yt6"
   },
   "source": [
    "Functions in the **``torch`` module**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7FKDL8ZG_yt2",
    "outputId": "d73484fe-f34f-49b0-c90d-135982c10bc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7645, 1.8300, 0.7657],\n",
       "        [1.9186, 0.7809, 1.2018],\n",
       "        [0.5131, 1.5873, 1.8815]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(t, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zD99wCT8_yt0"
   },
   "source": [
    "Tensors **methods**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_zkKPFHo_ytw",
    "outputId": "2e12ecba-3646-460d-88dd-0d0c3ef41069"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7645, 1.8300, 0.7657],\n",
       "        [1.9186, 0.7809, 1.2018],\n",
       "        [0.5131, 1.5873, 1.8815]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.add(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQ9ChdjH_ytv"
   },
   "source": [
    "#### **Basic operations and broadcasting**\n",
    "\n",
    "Basic mathematical operations $(+, -, *, /, **)$ are applied **elementwise or** do **broadcasting**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "doNfhKA5_ytq",
    "outputId": "8c315e86-5d5a-4361-d1de-2811fce7a251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]], dtype=torch.float64)\n",
      "tensor([[5.2000, 6.2000],\n",
      "        [7.2000, 8.2000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
    "y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float64)\n",
    "\n",
    "print(x + y)  # elementwise sum\n",
    "print(x + 4.2)  # broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "GlZG4i_D_ytj",
    "outputId": "829fa20b-3dce-4263-ae1b-4ee415fdac0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  7.],\n",
      "        [16., 27.]], dtype=torch.float64)\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# other examples\n",
    "print(x * y - 5)\n",
    "print(x - y / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rNTtIs2NMKt"
   },
   "source": [
    "Broadcasting is even more powerful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "4XzPdOKXNLV-",
    "outputId": "2e84adeb-15c5-4854-8474-74344cbaad97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([100,   0, 100]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[100,   1, 102],\n",
      "        [103,   4, 105],\n",
      "        [106,   7, 108],\n",
      "        [109,  10, 111]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "m = torch.arange(12).reshape(4, 3)\n",
    "v = torch.tensor([100, 0, 100])\n",
    "n = m + v\n",
    "print_arr(m, v, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "CYREKQAuQNYy",
    "outputId": "dee288dc-f48b-436f-c65f-2e10371daeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[ 0],\n",
      "        [10],\n",
      "        [ 0],\n",
      "        [10]]) <shape: torch.Size([4, 1])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[ 0,  1,  2],\n",
      "        [13, 14, 15],\n",
      "        [ 6,  7,  8],\n",
      "        [19, 20, 21]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "m = torch.arange(12).reshape(4, 3)\n",
    "u = torch.tensor([0, 10, 0, 10]).reshape(4,1)\n",
    "n = m + u\n",
    "print_arr(m, u, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ReNq-RtKg1Dy",
    "outputId": "0026da1e-7ea2-47de-c61a-6efaeeebb8c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0],\n",
      "        [10],\n",
      "        [ 0],\n",
      "        [10]]) <shape: torch.Size([4, 1])> <dtype: torch.int64>\n",
      "\n",
      "tensor([100,   0, 100]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[100,   0, 100],\n",
      "        [110,  10, 110],\n",
      "        [100,   0, 100],\n",
      "        [110,  10, 110]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "w = u + v\n",
    "print_arr(u, v, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYNysSHoQ6dX"
   },
   "source": [
    "Master broadcasting is very useful to write **vectorized** code, i.e. code that avoids explicit python loops which are so slow. \n",
    "\n",
    "Instead, this approach takes advantage of the underlying C implementation of PyTorch and Numpy (on CPU) or CUDA implementation of Pytorch (on GPU).\n",
    "\n",
    "![broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqyMVYPL_yoC"
   },
   "source": [
    "##### **EXERCISE**\n",
    ">\n",
    "> Given two vectors $X \\in R^n$ and $Y \\in R^m$ compute the differences between all possible pairs of numbers, and organize those differences in a matrix $Z \\in R^{n \\times m}$:\n",
    "> $$ z_{ij} = x_i - y_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WqFqMkksOBg"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "gtF2--mB_yn1"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀)\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5])\n",
    "out = x[:, None] - y[None, :]\n",
    "\n",
    "print_arr(x, y, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_L_GYRgsnLo"
   },
   "source": [
    "##### **EXERCISE**\n",
    ">\n",
    "> Given a  ${n \\times m}$ tensor and two indices $a \\in [0, n)$, $b \\in [0, m)$ and $p \\in [0, +\\inf)$,\n",
    "> create a new tensor $Y \\in R^{n \\times m}$ such that:\n",
    ">\n",
    "> $$ y_{ij} = d_{L_p}( (i,j), (a,b) ) \\text{ for each }  i \\in [0, n), j \\in  [0, m) $$\n",
    ">\n",
    "> That is, consider pairs of indices as points in $R^2$. Try different values of $p$ to see what happens.\n",
    ">\n",
    "> e.g. Using the $L_1$ distance given $(i,j) = (3, 5)$ and $(a,b) = (14, 20)$ we get:\n",
    "> $$ y_{3,5} = d_{L_1}( (3, 5), (14, 20) ) = |3 - 14| + |5 - 20| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdW4Xf964XQ1"
   },
   "outputs": [],
   "source": [
    "# Utility function\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_row_images(images: Union[torch.Tensor, np.ndarray]) -> None:\n",
    "  \"\"\" Plots the images in a subplot with multiple rows.\n",
    "\n",
    "  Handles correctly grayscale images.\n",
    "\n",
    "  :param images: tensor with shape [number of images, width, height, <colors>]\n",
    "  \"\"\"\n",
    "  from plotly.subplots import make_subplots\n",
    "  import plotly.graph_objects as go\n",
    "  fig = make_subplots(rows=1, cols=images.shape[0] ,\n",
    "                      specs=[[{}] * images.shape[0]])\n",
    "  \n",
    "  # Convert grayscale image to something that go.Image likes\n",
    "  if images.dim() == 3:\n",
    "    images = torch.stack((images, images, images), dim= -1)\n",
    "  elif (images.dim() == 4 and images.shape[-1] == 1):\n",
    "    images = torch.cat((images, images, images), dim= -1)\n",
    "\n",
    "  assert images.shape[-1] == 3 or images.shape[-1] == 4\n",
    "    \n",
    "  for i in range(images.shape[0]):  \n",
    "    i_image = np.asarray(images[i, ...])\n",
    "\n",
    "    fig.add_trace( \n",
    "        go.Image(z = i_image, zmin=[0, 0, 0, 0], zmax=[1, 1, 1, 1]),\n",
    "        row=1, col=i + 1\n",
    "    )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "# When using plotly pay attention that often it does not like PyTorch Tensors\n",
    "# ...and it does not give any error, just a empty plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApPt8XdAuK7R"
   },
   "outputs": [],
   "source": [
    "x = torch.zeros(300, 300)\n",
    "a = 150\n",
    "b = 150\n",
    "\n",
    "x[a, b] = 1  # Just to visualize the starting point\n",
    "plot_row_images(x[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFwiTbVV7iho"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "MUDYSUxLuoAK"
   },
   "outputs": [],
   "source": [
    "#@title Solution 1 (double click here to peek 👀)\n",
    "rows = torch.arange(x.shape[0])\n",
    "cols = torch.arange(x.shape[1])\n",
    "\n",
    "# Manual computation of L1\n",
    "y = (torch.abs(rows - a)[:, None] + torch.abs(cols - b)[None, :])\n",
    "px.imshow(y).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "PYF38G6dwAbT"
   },
   "outputs": [],
   "source": [
    "#@title Solution 2 (double click here to peek 👀)\n",
    "\n",
    "# Parametric computation of Lp\n",
    "p = 8\n",
    "y = ((torch.abs(rows - a ) ** p )[:, None] + \n",
    "     (torch.abs(cols - b) ** p)[None, :]) ** (1/p)\n",
    "px.imshow(y).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0O530uju9a0h"
   },
   "source": [
    "Solution 2 breaks with `p=10`. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "NYZfn5gJ5kOM"
   },
   "outputs": [],
   "source": [
    "#@title Follow-up solution (double click here to peek 👀)\n",
    "\n",
    "# This works even with p=10. Why?\n",
    "p = 100\n",
    "y = ((torch.abs(rows.double() - a ) ** p )[:, None] + \n",
    "     (torch.abs(cols.double() - b) ** p)[None, :]) ** (1/p)\n",
    "px.imshow(y).show()\n",
    "\n",
    "# ->\n",
    "p = 10\n",
    "#print(torch.tensor(10, dtype=torch.int) ** p)\n",
    "#print(torch.tensor(10, dtype=torch.double) ** p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezcEIBPFSlFa"
   },
   "source": [
    "##### **Broadcasting, let's take a peek under the hood**\n",
    "\n",
    "In short: if a PyTorch operation supports broadcast, then **its Tensor arguments can be automatically expanded to be of equal sizes** (without making copies of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfTF3SiYSlFg"
   },
   "source": [
    "###### **Broadcastable tensors**\n",
    "\n",
    "Two tensors are \"broadcastable\" if:\n",
    "- Each tensor has at least one dimension\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension **sizes** must either **be equal**, **one of them is 1**, or **one of them does not exist**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mdgd4qM5SlFh"
   },
   "source": [
    "###### **Broadcasting rules**\n",
    "\n",
    "Broadcasting two tensors together follows these rules:\n",
    "\n",
    "1. All input tensors have **1's prepended to their shapes**, to match the rank of the biggest tensor in input\n",
    "2. The size in each dimension of the **output shape** is the maximum of all the input sizes in that dimension\n",
    "3. An input can be used in the computation if its size in a particular **dimension either match** the output size in that dimension, **or has value exactly 1**\n",
    "4. If an input has a dimension size of 1 in its shape, the **first data entry in that dimension will be used for all calculations** along that dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcj42Be5SlFi"
   },
   "source": [
    "**In our example**:\n",
    "\n",
    "- `m` has shape (4,3)\n",
    "- `v` has shape (3,).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "lXQWIJtoSlFj",
    "outputId": "94cfdeb8-50c8-49f7-e216-9d7b786389ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([100,   0, 100]) <shape: torch.Size([3])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "print_arr(m, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZLeIamSSlFn"
   },
   "source": [
    "\n",
    "Following the Broadcasting logic, we can say the following is equivalent to what happened:\n",
    "\n",
    "- `v` has less dims than `m` so a dimension of `1` is **prepended** $\\to$ `v` is now `(1, 3)`.\n",
    "- Output shape will be `(max(1, 4), max(3, 3)) = (4, 3)`.\n",
    "- Dim 1 of `v` matches exactly (3); dim 0 is exactly 1, so we can use the first data entry in that dimension (i.e. the whole row 0 of `v`) for each time any row is accessed. This is effectively like converting `v` from `(1,3)` to `(4,3)` by replicating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeoux-PvSlFp"
   },
   "source": [
    "\n",
    "For more on broadcasting, see the [documentation](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "Functions that support broadcasting are known as universal functions (i.e. ufuncs). For Numpy you can find the list of all universal functions in the [documentation](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpE0yZGF_ytT"
   },
   "source": [
    "#### **Non-elementwise operations**\n",
    "\n",
    "\n",
    "PyTorch and NumPy provide many useful functions to perform computations on tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EI33i1Df_ytN",
    "outputId": "3f4d4455-423a-4881-db51-04756c8ef489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) <shape: torch.Size([2, 2])> <dtype: torch.float32>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "print_arr(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6x4rhtfI_ytI",
    "outputId": "99f67601-d7c8-4248-c961-e75d81ee486c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.) <shape: torch.Size([])> <dtype: torch.float32>\n"
     ]
    }
   ],
   "source": [
    "# Sum up all the elements\n",
    "print_arr(torch.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OvndASPe_ytD",
    "outputId": "319b8f9a-dfd1-4b7b-ae59-8c1459130ca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of each column\n",
    "print_arr(torch.mean(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUwZxvZP_ysy"
   },
   "source": [
    "> **REMEMBER!**\n",
    ">\n",
    "> In order to avoid confusion with the `dim` parameter, you can think of it as an index over the list returned by `tensor.shape`. The operation is performed iterating over that dimension.\n",
    "> \n",
    "> Visually: \n",
    "> \n",
    "><img src=\"https://qph.fs.quoracdn.net/main-qimg-30be20ab9458b5865b526d287b4fef9a\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K-4z5pL_ys-",
    "outputId": "a6f3e267-bad8-4f8d-c237-f3151acbd896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2., 12.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
     ]
    }
   ],
   "source": [
    "# Compute the product of each row\n",
    "print_arr(torch.prod(x, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MRtgzF33_ys4",
    "outputId": "94ce402b-091d-44d5-aa2e-c9eb8a98ccab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
     ]
    }
   ],
   "source": [
    "# Max along the rows (i.e. max value in each column)\n",
    "values, indices = torch.max(x, dim=0)\n",
    "print_arr(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hd0zbRp0_ys0",
    "outputId": "7e0fae4b-8d33-43ba-f53e-6c01a93f55f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
     ]
    }
   ],
   "source": [
    "# Max along the columns (i.e. max value in each row)\n",
    "values, indices = torch.max(x, dim=1)\n",
    "print_arr(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1xEs8jkkk4f"
   },
   "source": [
    "###### **Dim parameter, let's take a peek under the hood**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DoDvtWHq_ysu",
    "outputId": "0fa93f8b-3dca-4fe5-e79a-6c2459c02dca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5308, 2.6237, 1.7376],\n",
       "        [1.6753, 1.3749, 1.9190]])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 2\n",
    "\n",
    "a = torch.rand(2, 3, 4)\n",
    "out = a.sum(dim=dim)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9-KbxoTK_ysq",
    "outputId": "b074505e-603a-4d5c-c226-b85cb48833b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is summing over the `dim` dimension, i.e.:\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mZcG-q5R_ysm",
    "outputId": "e36c18fc-de05-4c88-c1ce-959e8dcbbdc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The `dim` dimension has 4 elements\n",
    "a.shape[dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zqT4jSkW_ysi",
    "outputId": "636cd05c-84fa-4bdc-efad-577ab91b2590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dimension dim collapses, the output tensor will have shape:\n",
    "new_shape = a.shape[:dim] + a.shape[dim + 1:]\n",
    "new_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GHakpxbl_ysd",
    "outputId": "35c8ee58-21bb-4b03-8a96-b283487f2e98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5308, 2.6237, 1.7376],\n",
       "        [1.6753, 1.3749, 1.9190]])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicitly compute the sum over dim\n",
    "out = torch.zeros(new_shape)                  \n",
    "for i in range(a.shape[dim]):\n",
    "  out += a.select(dim=dim, index=i)\n",
    "out\n",
    "\n",
    "# **DO NOT** use for loops in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQA3ngqoHsEt"
   },
   "source": [
    "##### **EXERCISE**\n",
    ">\n",
    "> Given a matrix $X \\in R^{k \\times k}$ compute the mean of the values along its diagonal. Perform this computation in at least two different ways, then check that the result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evQYg9-GH-Td"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(4, 4)\n",
    "print_arr(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3x8-6wyGcB_R"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "fOEhv8X0ILC2"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀)\n",
    "a = torch.mean(x[torch.arange(x.shape[0]), torch.arange(x.shape[1])])\n",
    "b = torch.sum(torch.eye(x.shape[0]) * x) / x.shape[0]\n",
    "c = torch.trace(x) / x.shape[0]\n",
    "d = torch.mean(torch.diag(x))\n",
    "\n",
    "print(torch.equal(a, b) and torch.equal(a, c) and torch.equal(a, d))\n",
    "print_arr(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YePwf4ok_ysb"
   },
   "source": [
    "##### **EXERCISE**\n",
    ">\n",
    "> Given a binary non-symmetric matrix $X \\in \\{0, 1\\}^{n, n}$, build the symmetric matrix $Y \\in \\{0, 1\\}^{n, n}$ defined as:\n",
    "> $$\n",
    "y_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x_{ij} = 1 \\\\\n",
    "1 & \\text{if } x_{ji} = 1 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$ \n",
    ">\n",
    "> *Hint*: search for `clamp` in the [docs](https://pytorch.org/docs/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Dv-OmnV_ysU"
   },
   "outputs": [],
   "source": [
    "x = torch.randint(0, 2, (5, 5))  # Non-symmetric matrix\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6HT6OSElDic"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "4tM6Yd14JrAB"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀)\n",
    "(x + x.t()).clamp(max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVC3YH8H_ysT"
   },
   "source": [
    "#### **Tensor contractions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QewYLJq-_yr5"
   },
   "source": [
    "##### **Matrix multiplication**\n",
    "\n",
    "Given $X \\in R^{n \\times d}$ and $Y \\in R^{d \\times v}$, their matrix multiplication $Z \\in R^{n \\times v}$ is defined as:\n",
    "\n",
    "$$ \\sum_{k=0}^{d} x_{ik} y_{kj} = z_{ij} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qu16frkd_yru",
    "outputId": "e8dba741-958d-4d47-ee7b-b7c49ac6fc7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]]) <shape: torch.Size([3, 2])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[1, 2],\n",
      "        [2, 1]]) <shape: torch.Size([2, 2])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "y = torch.tensor([[1, 2], [2, 1]])\n",
    "print_arr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PbVbXIzz_yrl",
    "outputId": "60806290-b5f0-43da-b016-1ec78a96a359"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  4],\n",
       "        [11, 10],\n",
       "        [17, 16]])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y  # Operator overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_FKQsMIG_yrh",
    "outputId": "4ecf450c-572a-4527-c403-14324e36a075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  4],\n",
       "        [11, 10],\n",
       "        [17, 16]])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x, y)  # Explicit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tTqIXFNU_yrZ",
    "outputId": "322406d0-4f75-423d-ba1f-809a5a3a3648"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  4],\n",
       "        [11, 10],\n",
       "        [17, 16]])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('ik, kj -> ij', (x, y))  # Einsum notation!\n",
    "\n",
    "# It summed up dimension labeled with the index `k`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-KPmcQg_ysS"
   },
   "source": [
    "##### **Dot product** \n",
    "Also known as Inner product. \n",
    "Given $x \\in R^k$ and $y \\in R^k$, the dot product $z \\in R$ is defined as:\n",
    "\n",
    "$$ \\sum_{i=0}^{k} x_i y_i = z $$\n",
    "\n",
    "Unlike MATLAB, ``*`` is the element wise multiplication, not the matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "6DxmdUoC_ysM",
    "outputId": "aee04aa1-cb4c-48d1-fc12-e8d09e0eb4e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([4, 5, 6]) <shape: torch.Size([3])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "print_arr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ixf_iXF3KbOv",
    "outputId": "b2726465-d2d6-4ecc-e3df-bdea1f3af889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to perform:\n",
    "(1 * 4) + (2 * 5) + (3 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c0jRMizc_ysG",
    "outputId": "f311efba-bbfc-4d05-c522-ae89eb65a113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(x, y)  # PyTorch explicit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GrW2utM9_ysA",
    "outputId": "f58de829-fd77-4fb6-9748-6d2867c48c35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y  # PyTorch operator overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xWV3hiAU_yr7",
    "outputId": "7b369dae-b52a-4e0c-ccdd-02c5887330a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i, i ->', (x, y))  # Einstein notation!\n",
    "\n",
    "# Multiply point-wise repeating indices in the input\n",
    "# Sum up along the indices that `do not` appear in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3a9Yl0IC_yrW"
   },
   "source": [
    "##### **Batch matrix multiplication**\n",
    "\n",
    "Often we want to perform more operations together. Why?\n",
    "- Reduce the **overhead of uploading** each tensor to/from the GPU memory\n",
    "- **Better parallelization** of the computation\n",
    "\n",
    "Given two 3D tensors, each one containing ``b`` matrices,\n",
    "$X \\in R^{b \\times n \\times m}$\n",
    "and  \n",
    "$Y \\in R^{b \\times m \\times p}$, \n",
    "\n",
    "We want to multiply together each $i$-th couple of matrices, obtaining a tensor $Z \\in R^{b \\times n \\times p}$ defined as:\n",
    "\n",
    "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{bkj} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "wwUMo2Br_yrQ",
    "outputId": "342af13c-10a7-43c5-c597-9c8170977725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]]]) <shape: torch.Size([2, 3, 2])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[[1, 2],\n",
      "         [2, 1]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [2, 1]]]) <shape: torch.Size([2, 2, 2])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n",
    "y = torch.tensor([[[1, 2], [2, 1]], [[1, 2], [2, 1]]])\n",
    "print_arr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "FDW_9XKJ_yrG",
    "outputId": "894d3993-01f4-410e-ca0d-cf9a2a5cbd38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]],\n",
       "\n",
       "        [[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(x, y)  # **not** torch.mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "zf7GaNmw_yrM",
    "outputId": "bc37ce15-0fe8-4886-f63d-b9b533b6c102"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]],\n",
       "\n",
       "        [[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "wjjRPx-n_yrC",
    "outputId": "ce808867-862b-4c16-b85e-593422d78fcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]],\n",
       "\n",
       "        [[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('bik, bkj -> bij', (x, y)) # Einstein notation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aC7y2peia8mp"
   },
   "source": [
    "Can you feel the power of Einstein notation vibrating off of you?\n",
    "\n",
    "\n",
    "![Surfing einstein](https://roma.corriere.it/methode_image/2019/05/13/Roma/Foto%20Roma%20-%20Trattate/einstein2-kZfE-U3120295526975hVE-656x492@Corriere-Web-Roma.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-aYQyHz_yrA"
   },
   "source": [
    "##### **Broadcast matrix multiplication**\n",
    "\n",
    "Given $b$ matrices with dimensions $n \\times m$ organized in one 3D tensor $X \\in R^{b \\times n \\times m}$\n",
    "and one 2D tensor $Y \\in R^{m \\times p}$, \n",
    "\n",
    "We want to multiply together each matrix $X_{i,:,:}$ with $Y$, obtaining a tensor $Z \\in R^{b \\times n \\times p}$ defined as:\n",
    "\n",
    "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{kj} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "awE5anPp_yq7",
    "outputId": "ab14578c-3280-44d4-e793-e9b5a835e67c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]]]) <shape: torch.Size([2, 3, 2])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[1, 2],\n",
      "        [2, 1]]) <shape: torch.Size([2, 2])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n",
    "y = torch.tensor([[1, 2], [2, 1]])\n",
    "print_arr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "L7fxtm64_yq3",
    "outputId": "002a843c-34ed-4f56-d19a-cc421215abdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]],\n",
       "\n",
       "        [[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y   # Operator overload: always use the last two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "fvZyCiYN_yqz",
    "outputId": "097d53c1-2db1-41a0-fc1e-2b2dd96308e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]],\n",
       "\n",
       "        [[ 5,  4],\n",
       "         [11, 10],\n",
       "         [17, 16]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, y)  # Explicit PyTorch API: always use the last two dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iod-0Z04_yqx"
   },
   "source": [
    "##### **EXERCISE**\n",
    ">\n",
    "> Use the einsum notation to compute the equivalent broadcast matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsgxdJnP_yqv"
   },
   "source": [
    "##### **Einsum notation**\n",
    "\n",
    "Einstein notation is a way to express complex operations on tensors\n",
    "\n",
    "- It is concise but enough expressive to do almost every operation you will need in building your neural networks, letting you think on the only thing that matters... **dimensions!**\n",
    "- You will not need to check your dimensions after an einsum operation, since the dimensions themselves are *defining* the tensor operation.\n",
    "-  You will not need to explicitly code intermediate operations such as reshaping, transposing and intermediate tensors\n",
    "- It is not library-specific, being avaiable in ``numpy``, ``pytorch`` and ``tensorflow`` with the same signature. So you do not need to remember the functions signature in all the frameworks.\n",
    "- Can sometimes be compiled to high-performing code (e.g. [Tensor Comprehensions](https://pytorch.org/blog/tensor-comprehensions/))\n",
    "\n",
    "Check [this blog post by Olexa Bilaniuk](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/) to take a peek under the hood of einsum and [this one by Tim Rocktäschel](https://rockt.github.io/2018/04/30/einsum) to look at even more examples than the ones that follows.\n",
    "\n",
    "Its formal behaviour is well described in the [Numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n",
    "However, it is very intuitive and better explained through examples.\n",
    "\n",
    "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-fmtstring.png?w=676)\n",
    "\n",
    "> *Historical note (taken from the Bilaniuk post)*\n",
    ">\n",
    "> Einstein had no part in the development of this notation. He merely popularized it, by expressing his entire theory of General Relativity in it. In a letter to [Tullio Levi-Civita](https://en.wikipedia.org/wiki/Tullio_Levi-Civita), co-developer alongside [Gregorio Ricci-Curbastro](https://en.wikipedia.org/wiki/Gregorio_Ricci-Curbastro) of Ricci calculus (of which this summation notation was only a part), Einstein wrote:\n",
    ">\n",
    "> \" *I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.* \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdyM0vLB_yqq"
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ow4puwv4_yqp"
   },
   "source": [
    "###### **Matrix transpose**\n",
    "\n",
    "$$ B_{ji} = A_{ij} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "LmE5nSrt_yqk",
    "outputId": "ea53904e-09e6-494b-ce2c-f16e1342b9aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]]) <shape: torch.Size([3, 2])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# The characters are indices along each dimension\n",
    "b = torch.einsum('ij -> ji', a)\n",
    "print_arr(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z8j-iVH_yqi"
   },
   "source": [
    "###### **Sum**\n",
    "\n",
    "$$ b = \\sum_i \\sum_j A_{ij} := A_{ij} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "HFjp7cOb_yqd",
    "outputId": "f58f6ba4-8394-47e0-f2fb-aa1532f41b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor(15) <shape: torch.Size([])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# Indices that do not appear in the output tensor are summed up\n",
    "b = torch.einsum('ij -> ', a)\n",
    "print_arr(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hks-z_NN_yqb"
   },
   "source": [
    "###### **Column sum**\n",
    "\n",
    "$$ b_j = \\sum_i A_{ij} := A_{ij} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "MXbTLNtL_yqX",
    "outputId": "00e0829b-ed7b-44d0-b851-e49bdd9ceb44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([3, 5, 7]) <shape: torch.Size([3])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# Indices that do not appear in the output tensor are summed up,\n",
    "# even if some other index appears\n",
    "b = torch.einsum('ij -> j', a)\n",
    "print_arr(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1KQbgD-OEu4"
   },
   "source": [
    "###### **EXERCISE**\n",
    ">\n",
    "> Which will be the shape and type of the following tensor $X \\in R^{100 \\times 200}$? Which values will it contain? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdBJvJt0OLsB"
   },
   "outputs": [],
   "source": [
    "x = (torch.rand(100, 200) > 0.5).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDSB2iy0Mjf3"
   },
   "source": [
    "###### **EXERCISE** \n",
    ">\n",
    "> Given a binary tensor $X \\in \\{0, 1\\}^{n \\times m}$ return a tensor $y \\in R^{n}$ that has in the $i$-th position the **number of ones** in the $i$-th row of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sewLwDHNdQq"
   },
   "outputs": [],
   "source": [
    "# Display a binary matrix with plotly\n",
    "\n",
    "fig = px.imshow(x)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-YO6eOmrUkK"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "iVQEYEThQn-I"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀) \n",
    "# Count the number of ones in each row\n",
    "row_ones = torch.einsum('ij -> i', x)\n",
    "\n",
    "row_ones2 = torch.sum(x, dim=-1)\n",
    "\n",
    "torch.equal(row_ones, row_ones2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUlzmmfTQ_p8"
   },
   "outputs": [],
   "source": [
    "px.imshow(row_ones[:, None]).show()\n",
    "print(f'Sum up the row counts: {row_ones.sum()}\\nSum directly all the ones in the matrix: {x.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JA5cWXv3_yqN"
   },
   "source": [
    "###### **Matrix-vector multiplication**\n",
    "\n",
    "$$ c_i = \\sum_k A_{ik}b_k := A_{ik}b_k $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "zYF4uNUC_yqJ",
    "outputId": "0482099e-1756-43f9-b761-d32888fb823a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([0, 1, 2]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([ 5, 14]) <shape: torch.Size([2])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# Repeated indices in different input tensors indicate pointwise multiplication\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(3)\n",
    "c = torch.einsum('ik, k -> i', [a, b])  # Multiply on k, then sum up on k\n",
    "print_arr(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDsla37O_yqH"
   },
   "source": [
    "###### **Matrix-matrix multiplication**\n",
    "\n",
    "$$ C_{ij} = \\sum_k A_{ik}B_{kj} := A_{ik}B_{kj} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8khafOAKElM"
   },
   "source": [
    "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "7mcYlmu5_yqD",
    "outputId": "bb5da5e4-364e-4b67-eb90-90c1a49338fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]]) <shape: torch.Size([3, 5])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[ 25,  28,  31,  34,  37],\n",
      "        [ 70,  82,  94, 106, 118]]) <shape: torch.Size([2, 5])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(15).reshape(3, 5)\n",
    "c = torch.einsum('ik, kj -> ij', [a, b])\n",
    "print_arr(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVAJP6Ma_yqC"
   },
   "source": [
    "###### **Dot product multiplication**\n",
    "\n",
    "$$ c = \\sum_i a_i b_i := a_i b_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "2x-XwGOy_yp6",
    "outputId": "cf0bd205-d4f1-4e7b-d920-cd13cb394845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([3, 4, 5]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
      "\n",
      "tensor(14) <shape: torch.Size([])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3)\n",
    "b = torch.arange(3,6) \n",
    "c = torch.einsum('i,i->', (a, b))\n",
    "print_arr(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhLZ4Gl__yp4"
   },
   "source": [
    "###### **Point-wise multiplication**\n",
    "Also known as hadamard product\n",
    "\n",
    "$$ C_{ij} = A_{ij} B_{ij} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "QTUH61Ft_yp0",
    "outputId": "66c7ef93-a287-4130-9c34-d55c0fade8d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[ 0,  7, 16],\n",
      "        [27, 40, 55]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6,12).reshape(2, 3)\n",
    "c = torch.einsum('ij, ij -> ij', (a, b))\n",
    "print_arr(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZoiSCsn_ypz"
   },
   "source": [
    "###### **Outer product**\n",
    "\n",
    "$$ C_{ij} = a_i b_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "k71BkJbf_ypu",
    "outputId": "eb38b9b3-e313-4367-ab6e-c07a96fdc0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
      "\n",
      "tensor([3, 4, 5, 6]) <shape: torch.Size([4])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[ 0,  0,  0,  0],\n",
      "        [ 3,  4,  5,  6],\n",
      "        [ 6,  8, 10, 12]]) <shape: torch.Size([3, 4])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3)\n",
    "b = torch.arange(3,7)\n",
    "c = torch.einsum('i, j -> ij', (a, b))\n",
    "print_arr(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "1MQYFeM2_ypo",
    "outputId": "df3bdabc-4a4d-4bd2-f3e1-9bfd7de0ae35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 3,  4,  5,  6],\n",
       "        [ 6,  8, 10, 12]])"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the standard PyTorch API\n",
    "torch.ger(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXQHwUet_yph"
   },
   "source": [
    "###### **Batch matrix multiplication**\n",
    "\n",
    "$$ c_{bij} = \\sum_k a_{bik} b_{bkj} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "6vpbY37H_ypb",
    "outputId": "ad317063-9a8b-45a6-865d-d60b7b2450ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8712, -0.2234,  1.7174,  0.3189,  1.1914],\n",
      "         [-0.8140, -0.7360, -0.8371, -0.9224,  1.8113]],\n",
      "\n",
      "        [[ 0.1606,  0.3672,  1.8446, -1.1845,  1.3835],\n",
      "         [-1.2024,  0.7078, -1.0759,  0.5357,  1.1754]]]) <shape: torch.Size([2, 2, 5])> <dtype: torch.float32>\n",
      "\n",
      "tensor([[[-1.9733e-01, -1.0546e+00,  1.2780e+00],\n",
      "         [ 1.4534e-01,  2.3105e-01,  8.6540e-03],\n",
      "         [-1.4229e-01,  1.9707e-01, -6.4172e-01],\n",
      "         [-2.2064e+00, -7.5080e-01,  2.8140e+00],\n",
      "         [ 3.5979e-01, -8.9808e-02,  2.8647e-02]],\n",
      "\n",
      "        [[ 6.4076e-01,  5.8325e-01,  1.0669e+00],\n",
      "         [-4.5015e-01, -6.7875e-01,  5.7432e-01],\n",
      "         [ 1.8775e-01,  1.7847e-01,  2.6491e-01],\n",
      "         [ 1.2732e+00, -1.3109e-03, -3.0360e-01],\n",
      "         [-9.8644e-01,  1.2330e-01,  3.4987e-01]]]) <shape: torch.Size([2, 5, 3])> <dtype: torch.float32>\n",
      "\n",
      "tensor([[[-0.3798,  0.8592, -1.2860],\n",
      "         [ 2.8596,  1.0533, -3.0532]],\n",
      "\n",
      "        [[-2.5890,  0.3457,  1.7146],\n",
      "         [-1.7685, -1.2295, -0.9128]]]) <shape: torch.Size([2, 2, 3])> <dtype: torch.float32>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2,5)\n",
    "b = torch.randn(2,5,3)\n",
    "c = torch.einsum('ijk,ikl->ijl', [a, b])\n",
    "print_arr(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fm8KVyoyoNh"
   },
   "source": [
    "###### **EXERCISE**\n",
    ">\n",
    "> - Matrix transpose with einsum ($Y = M^T$)\n",
    "> - Quadratic form with einsum  ($y = v^TMv$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKMXdwUdRN4U"
   },
   "source": [
    "#### Singleton dimensions\n",
    "\n",
    " It is very common to **add or remove dimensions of size $1$** in a tensor.\n",
    "\n",
    " It is possible to perform these operations in different ways, feel free to use\n",
    " whatever is more comfortable to you.\n",
    "\n",
    " e.g. If we want to transform a rank-1 tensor into a rank-2 column tensor and the back to a rank-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3DsWB2_iRwTg",
    "outputId": "527a1a8c-01a1-4ba3-bdf7-a8d6ce362b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# Define a rank-1 tensor\n",
    "x = torch.arange(6)\n",
    "print_arr(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFqraseqSAhO"
   },
   "source": [
    "Transform **`x` into a column tensor** in tree different ways.\n",
    "\n",
    "Remember that the shape of a column tensor is in the form: `(rows, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "u-BC2G0TSSfO",
    "outputId": "f2f8d7d4-a9e2-4a34-da8f-e9ccf54d0569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# Use the `reshape` or `view` functions\n",
    "\n",
    "y1 = x.reshape(-1, 1)\n",
    "y2 = x.view(-1, 1)\n",
    "\n",
    "print_arr(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "eMOA51cBPp1i",
    "outputId": "f72427ed-5f75-402a-91c7-d1652c7cc6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n",
      "\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# Use the specific `unsqueeze` function to unsqueeze a dimension\n",
    "\n",
    "y3 = x.unsqueeze(dim=-1)\n",
    "y4 = x.unsqueeze(dim=1)\n",
    "\n",
    "print_arr(y3, y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "GXkhm2nVN8KZ",
    "outputId": "9557eb3b-dfa7-437d-9f0e-96a42e1fecf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# Explicitly index a non-exixtent dimension with `None`\n",
    "\n",
    "y5 = x[:, None]\n",
    "\n",
    "print_arr(y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "2XBTG4CNTcLf",
    "outputId": "fdff2b44-e978-423c-9208-e35266a5e999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n"
     ]
    }
   ],
   "source": [
    "# To go back into a rank-1 tensor\n",
    "\n",
    "x1 = y1.reshape(-1)\n",
    "x2 = y2.view(-1)          # Explicity enforce to get a view of the tensors, without copying data\n",
    "x3 = y3.squeeze(dim=-1)\n",
    "x4 = y4.squeeze(dim=1)\n",
    "x5 = y5[:, 0]             # Manually collapse the dimension with an integer indexing\n",
    "\n",
    "print_arr(x1, x2, x3, x4, x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMVrjaXWySWR"
   },
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> indexing with `...` means  **keeps all the other dimension the same**.\n",
    "> Keep in mind that `...` is just a Python singleton object (just as `None`). \n",
    "> Its type is Ellipsis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MNKIfvMTsipK",
    "outputId": "9728f34d-4343-4b01-ea68-769ebbd1f1e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "BErfK7zVFhAV",
    "outputId": "55c64aca-f3d2-4a81-d94b-51f137bed128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3336, 0.2846, 0.3139],\n",
       "        [0.1568, 0.1054, 0.9302],\n",
       "        [0.8460, 0.0850, 0.3908]])"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3,3,3)\n",
    "x[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Eo3ymGK7FhZS",
    "outputId": "c3b2d05f-22ba-4b2f-f064-72c7690a9994"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3336, 0.2846, 0.3139],\n",
       "        [0.1568, 0.1054, 0.9302],\n",
       "        [0.8460, 0.0850, 0.3908]])"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[..., 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKyFGa7B6jXb"
   },
   "source": [
    "### Tensor types\n",
    "Pay attention to the tensor types!\n",
    "Several methods are available to convert tensors to different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHIa9x_X6tnE"
   },
   "outputs": [],
   "source": [
    "a = torch.rand(3, 3) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "5swdKvAa6w81",
    "outputId": "097c0411-e6a8-43d8-f8c6-39e0d6e8d215"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 1, 1],\n",
       "        [1, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "XQfh53W26x6R",
    "outputId": "38d6ef8b-1b34-403d-d660-215156ed6dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 1, 1],\n",
       "        [1, 0, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "22VBQVrG64Qq",
    "outputId": "e91915e8-be19-4bd2-fd65-d18559c1d5e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3712, 0.6330, 0.9137],\n",
       "        [1.1044, 1.2581, 1.4037],\n",
       "        [1.4555, 0.6035, 1.1258]])"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WSdMzMOb6yk0",
    "outputId": "f5287683-5be2-4aad-e3c1-db3514787696"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3712, 0.6330, 0.9137],\n",
       "        [1.1044, 1.2581, 1.4037],\n",
       "        [1.4555, 0.6035, 1.1258]], dtype=torch.float64)"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "rXKThOrW6zQF",
    "outputId": "3b4ff03b-d7ce-4968-cd2c-2ab160c03f8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "BLuyxzT_60Bw",
    "outputId": "64775f01-cd45-44e6-b523-cc280049b3ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3712, 0.6330, 0.9137],\n",
       "        [1.1044, 1.2581, 1.4037],\n",
       "        [1.4555, 0.6035, 1.1258]], dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sJDsriTC64-e",
    "outputId": "c41ae63a-6417-4437-a900-4f9ebef02c1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 1, 1],\n",
       "        [1, 0, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "_YRWAuTW7Ybn",
    "outputId": "7ef8e82a-4b67-4ebf-efdd-635dc70785c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bool().int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uu0xWOBV_yny"
   },
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7yW6kZ7_ynq"
   },
   "source": [
    "Do not try to memorize all the PyTorch API: \n",
    "> Learn to understand what operation should already exist and search for it, when you need it.\n",
    "\n",
    "Google, StackOverflow and the documentation are your friends!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lafVikgz_ypX"
   },
   "source": [
    "### Final exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPeJMqEACs8z"
   },
   "source": [
    "\n",
    "#### **EXERCISE 1**\n",
    ">\n",
    "> You are given $b$ images with dimensions $w \\times h$. Each pixel in each image has an `(r, g, b)` $c$ channel. These images are organized in a tensor $X \\in R^{w \\times b \\times c \\times h}$.\n",
    ">\n",
    "> You want to apply a linear trasformation to the color channel of each single image. In particular, you want to convert each image into a grey scale image.\n",
    "> Afterthat, transpose the images.\n",
    ">\n",
    "> The linear traformation that converts from `(r, g, b)` to grey scale is simply a linear combination of `r`, `g` and `b`. It can be encoded in the following 1-rank tensor $y \\in R^3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BRybv63oysl"
   },
   "outputs": [],
   "source": [
    "y = torch.tensor([0.2989, 0.5870, 0.1140], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ON5HBjD2oyLi"
   },
   "source": [
    "\n",
    "> You want to obtain a tensor $Z \\in R^{b \\times w \\times h \\times 3}$.\n",
    "> \n",
    "> Write the PyTorch code that performs this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "If137gBApDcR"
   },
   "outputs": [],
   "source": [
    "# Create the input tensor for the exercise\n",
    "\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "image1 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Earth_Eastern_Hemisphere.jpg/260px-Earth_Eastern_Hemisphere.jpg')\n",
    "image1 = torch.from_numpy(resize(image1, (300, 301), anti_aliasing=True)).float()  # Covert  to float type\n",
    "image1 = image1[..., :3]  # remove alpha channel\n",
    "\n",
    "image2 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg/628px-The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg')\n",
    "image2 = torch.from_numpy(resize(image2, (300, 301), anti_aliasing=True)).float()\n",
    "image2 = image2[..., :3]  # remove alpha channel\n",
    "\n",
    "image3 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Wikipedia-logo-v2.svg/1920px-Wikipedia-logo-v2.svg.png')\n",
    "image3 = torch.from_numpy(resize(image3, (300, 301), anti_aliasing=True)).float()\n",
    "image3 = image3[..., :3]  # remove alpha channel\n",
    "\n",
    "source_images = torch.stack((image1, image2, image3), dim=0)\n",
    "images = torch.einsum('bwhc -> wbch', source_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGgBC-dkqwVo"
   },
   "outputs": [],
   "source": [
    "# Plot source images\n",
    "plot_row_images(source_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Flm4DsjMww90"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "NwGXAyrVryTz"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀) \n",
    "\n",
    "# Grey-fy all images together, using the `images` tensor\n",
    "gray_images = torch.einsum('wbch, c -> bwh', (images, y))\n",
    "\n",
    "# What if you want to transpose the images?\n",
    "gray_images_tr = torch.einsum('wbch, c -> bhw', (images, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEWp-NDzsB4V"
   },
   "outputs": [],
   "source": [
    "# Plot the gray images\n",
    "plot_row_images(gray_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ga_3wk7BPmp6"
   },
   "outputs": [],
   "source": [
    "# Plot the gray transposed images\n",
    "plot_row_images(gray_images_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1tHmuIBM9ka"
   },
   "source": [
    "#### **EXERCISE 2**\n",
    ">\n",
    ">  You are given $b$ images with dimensions $w \\times h$. Each pixel in each image has an `(r, g, b)` $c$ channel. These images are organized in a tensor $X \\in R^{w \\times b \\times c \\times h}$, i.e. the same tensor as in the previous exercise.\n",
    ">\n",
    "> You want to swap the `red` color with the `blue` color, and decrese the intensity of the `green` by half\n",
    ">\n",
    "> Perform the transormation on all the images together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lppGxeMSO0c8"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POWStC54w2ZZ"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "VE2xc7i_ORzg"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀) \n",
    "\n",
    "# Define the linear tranformation to swap the blue and red colors\n",
    "S = torch.tensor([[ 0, 0, 1],\n",
    "                  [ 0, .5, 0],\n",
    "                  [ 1, 0, 0]], dtype=torch.float)\n",
    "\n",
    "# Apply the linear transformation to the color channel!\n",
    "rb_images = torch.einsum('wbch, dc -> bwhd', (images, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zdfYOMBOxjv"
   },
   "outputs": [],
   "source": [
    "plot_row_images(rb_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oF6FdNroYMqn"
   },
   "source": [
    "#### **EXERCISE 3**\n",
    ">\n",
    "> Given $k$ points organized in a tensor $X \\in R^{k \\times 2}$ apply a reflection along the $y$ axis as a linear transformation.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_d9KnJ5Y3e0"
   },
   "outputs": [],
   "source": [
    "# Define some points in R^2\n",
    "x = torch.arange(100, dtype=torch.float)\n",
    "y = x ** 2\n",
    "\n",
    "# Define some points in R^2\n",
    "data = torch.stack((x, y), dim=0).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0_JB4C1Y2vm"
   },
   "outputs": [],
   "source": [
    "px.scatter(x = data[:, 0].numpy(), y = data[:, 1].numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1Xt5XEkDYvB"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "eWCEwmIfaebY"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀) \n",
    "\n",
    "# Define a matrix that encodes a linear transformation\n",
    "S = torch.tensor([[-1, 0],\n",
    "                  [ 0, 1]], dtype=torch.float)\n",
    "\n",
    "# Apply the linear transformation: the order is important\n",
    "new_data = torch.einsum('nk, dk -> nd', (data, S))\n",
    "\n",
    "# The linear transformation correclty maps the basis vectors!\n",
    "S @ torch.tensor([[0],\n",
    "                  [1]], dtype=torch.float)\n",
    "S @ torch.tensor([[1],\n",
    "                  [0]], dtype=torch.float)\n",
    "\n",
    "# Check if at least the shape is correct\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5H7VEJFXbaMb"
   },
   "outputs": [],
   "source": [
    "# Plot the new points\n",
    "px.scatter(x = new_data[:, 0].numpy(), y = new_data[:, 1].numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLCQReiBPLqd"
   },
   "source": [
    "#### **EXERCISE 4**\n",
    ">\n",
    ">  You are given $b$ images with dimensions $w \\times h$. Each pixel in each image has an `(r, g, b)` $c$ channel. These images are organized in a tensor $X \\in R^{w \\times b \\times c \\times h}$, i.e. the same tensor as exercise 1 and 2.\n",
    ">\n",
    "> You want to convert each image into a 3D point cloud, where the `(x, y)`  coordinates are the indices of the pixels, and the `z` coordinate is the $L_2$ norm of the color of each pixel, multiplied by $10$\n",
    ">\n",
    "> *Hint*: you may need some other PyTorch function, search the docs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxLy9MHNC26V"
   },
   "outputs": [],
   "source": [
    "# ✏️ your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Sk-O_iHgSlbX"
   },
   "outputs": [],
   "source": [
    "#@title Solution (double click here to peek 👀) \n",
    "\n",
    "# Just normalize the tensor into the common form [batch, width, height, colors]\n",
    "imgs = torch.einsum('wbch -> bwhc', images)\n",
    "imgs.shape\n",
    "\n",
    "# The x, y coordinate of the point cloud are all the possible pairs of indices (i, j)\n",
    "row_indices = torch.arange(imgs.shape[1], dtype=torch.float)\n",
    "col_indices = torch.arange(imgs.shape[2], dtype=torch.float)\n",
    "xy = torch.cartesian_prod(row_indices , col_indices)\n",
    "\n",
    "# Compute the L2 norm for each pixel in each image\n",
    "depth = imgs.norm(p=2, dim = -1)\n",
    "# depth = torch.einsum('bwhc, bwhc -> bwh', (imgs, imgs)) ** (1/2)\n",
    "\n",
    "# For every pair (i, j), retrieve the L2 norm of that pixel\n",
    "z = depth[:, xy[:, 0].long(), xy[:, 1].long()] * 10\n",
    "\n",
    "# Adjust the dimensions, repeat and concatenate accordingly\n",
    "xy = xy[None, ...]\n",
    "xy = xy.repeat(imgs.shape[0], 1, 1)\n",
    "z = z[..., None] \n",
    "clouds = torch.cat((xy, z), dim= 2)\n",
    "\n",
    "clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1S0OYtiZgT_"
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def plot_3d_point_cloud(cloud: Union[torch.Tensor, np.ndarray]) -> None:\n",
    "  \"\"\" Plot a single 3D point cloud\n",
    "\n",
    "  :param cloud: tensor with shape [number of points, coordinates]\n",
    "  \"\"\"\n",
    "  import pandas as pd\n",
    "  df = pd.DataFrame(np.asarray(cloud), columns=['x', 'y', 'z'])\n",
    "  fig = px.scatter_3d(df, x=df.x, y=df.y, z=df.z, color=df.z, opacity=1, range_z=[0, 30])\n",
    "  fig.update_layout({'scene_aspectmode': 'data', 'scene_camera':  dict(\n",
    "          up=dict(x=0., y=0., z=0.),\n",
    "          eye=dict(x=0., y=0., z=3.)\n",
    "      )})\n",
    "  fig.update_traces(marker=dict(size=2,),\n",
    "                    selector=dict(mode='markers'))\n",
    "  _ = fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlKzABIKm9UZ"
   },
   "outputs": [],
   "source": [
    "plot_3d_point_cloud(clouds[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5Jm-zYSbQYU"
   },
   "outputs": [],
   "source": [
    "plot_3d_point_cloud(clouds[1, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH0CITRgZnyZ"
   },
   "outputs": [],
   "source": [
    "plot_3d_point_cloud(clouds[2, ...])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02.Tensor operations.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
